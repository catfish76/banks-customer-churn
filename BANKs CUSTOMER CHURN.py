#!/usr/bin/env python
# coding: utf-8

# # Project Title : BANK CUSTOMER CHURN ANALYSIS

# In[ ]:





# ## MY PROFILE

# In[1]:


from PIL import Image


# In[2]:


img=Image.open('Customer-Churn-1.png')


# In[3]:


img


# In[ ]:





# ### INTRODUCTION TO LOGISTIC REGRESSION 

# ### Logistic regresion makes it possible to predict a variable that is  dichotomous ,meaning a prediction that requires only two variable.eg a yes or no answer , true or false , big or small, man or woman etc.
# ###   The varable to be resolve is called , predicted , target or criterion ,while the varables you need to predict the ###results are,called the independent variables, or the predictors.
#  ### Examples of different machine learning logistic regression algorithms are ,
# ### Naive Bayes
# ###  Logistic Regression
# ### K-Nearest Neighbours
# ### Support Vector Machine
# ### Decision Tree
# ### Bagging Decision Tree (Ensemble Learning I)
# ### Boosted Decision Tree (Ensemble Learning II)
# ### Random Forest (Ensemble Learning III)
# ### Voting Classification (Ensemble Learning IV)
# ### Neural Network (Deep Learning) etc and we will be using some of them in this project
# 

# ## AIMS AND OBJECTIVES OF CARRYING OUT  CUSTOMER CHURN ANALYSIS:
# 
# 

# #### Customer churn is the rate at which customers unsbscribe or leave a company .It can be for a virety of reason like ,
# #### unpleasant customer service , increase in cost beared by customers , un updated technologies and many other reasons.
# ####   Customers are inevitably the most important part of any organisation, the lost of a customer simply means reduction in
# #### revenue , which can lead to slow business growth in a company or business and in most cases , constant increase in customer 
# #### churn, may lead to outright closure of the company or business.
# ####     Hence customer churn analysis and building the right model for a business to avert customer churn ,is a critical and very 
# #### important aspect of every company ,knowing the right reason a company customer churn and very early can help the company ,
# #### adress the issue effectively and stop any future customers churn timely.
# ####       customer churn also help companies to  be able to segment customers in groups reduce marketing expences , by understanding  how to target the right potential customers  through  targeted marketing etc , it also gives companies ,opportunity to understand their customers needs better.
# 
#         
#     

# ## PROBLEM STATEMENT:

# ### An old bank have  discovered that , every month, the number of V.I.P , cutomers unsubscribing and leaving their bank monthly , despite the bank  reducing intrest rate to encourage customers and  also introducing a lot of new automatic bank transaction packages at no extra cost ,to encourage customers to stay with the bank ,have no effect , as  more customers are leaving the bank, than the number of new customers subscribing to the bank .
# 
# ### Further preliminary analsis shows the following 
# ### 1, The cost of getting new customers to subscribe to the bank is 25% more expensive than retaining old customers, 
# 
# ### 2, Most new customers that subscribe, replacing the V.I.P. customers are  average customers with low income ,hence low savings balance in their account. which is generally reducing the  total income generated by the bank monthly.
# 
# ### Through another analysis the bank also determined that ,if this customer churn trend continue at this rate monthly , the bank may become bankrupt in 24 months peeriod and needed a fast and permanent solution to these higlighted problems.
# 
# 
# ### The bank have contacted me to determine the following and advice them accordignly:
# ### a, Why are customers churning  on the increase monthly ,especially the  V.I.P. customers
# ### b,Which insights can be derived from my analysis to highlight the main problems
# ### c, Advice the bank managers or management on how to solve this problems , based on insights derived from the customer transaction data provided to me , by the bank.
# ### d, The bank also want me to build a solution  model ,that can be used by the bank to prevent this issue in the future.

# ## BENEFICARIES OF THIS ANALYSIS

# ### 1, SALES DEPARTMENT
# ### 2, CUSTOMER SERVICE
# ### 3,MARKETING DEPARTMENT
# ### 4,PRODUCTION : GOODS/SERVICES
# ### 5, LOGISTICS 
# ### 6, COMPANY MANAGEMENT 

# ## APPROACH :

# ### 1, UNDERSTANDING THE PROBLEM STATEMENT
# ### 2, GATHERING OF NECESSARY DATA FOR ANALYSIS AND MODEL DEPLOYMENT
# ### 3, STUDYING AND UNDERSTANDING THE DATA
# ### 4, CLEANING / WRANGLING  THE DATA
# ### 5, QUERRYING AND PERFORMING ANALYSIS ON THE DATA WITH VISUALIZATIONS
# ### 6, FUTURE ENGINEERING / DATA PREPROCESSING
# ### 7, SPLITING THE DATA 
# ### 8,APPLYING RELEVANT LOGISTIC REGRESSION MACHINE LEARNING ALGORITHMS
# ### 9, DETERMINING THE RESULTS BEST ACCURACY
# ### 10 BUILDING AND DEPLOYING THE MODEL FOR FUTURE PRODUCTION 

# ## 1, IMPORTING THE PYTHON LYBRARIES AND  DATASET :

# In[4]:


import pandas as pd
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import warnings
warnings.filterwarnings("ignore")
import itertools
import missingno as msno
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
pd.options.display.float_format = '{:.2f}'.format
import warnings
warnings.filterwarnings('ignore')        


# In[5]:


churn1=pd.read_csv('C:/Users/HP/Downloads/churn_modelling1.csv')
                  


# In[6]:


churn1.head()


# In[7]:


churn2=pd.read_csv('C:/Users/HP/Downloads/churn_modelling2.csv') 


# In[8]:


churn2.head()


# ### COLUMN NAME CONSISTENT

# In[514]:


# USEING RENAME FUNCTION TO MAKE ALL COLUMNS NAME CONSISTENT
churn2.columns


# In[10]:


churn2.rename(columns={'Row_Number':'RowNumber','Has_CrCard':'HasCrCard'},inplace = True)


# In[11]:


churn2.rename(columns={'    Estimated  Salary':'  estimated salary'},inplace=True)


# In[12]:


churn2.columns


# In[13]:


churn2.head()


# ### USE STR REPLACE TO REPLACE SPACES  BEFORE OR AFTER THE COLUMN NAME

# In[516]:


churn2.columns=churn2.columns.str.replace(' ','')


# ### WE CAN REPLACE COLUMN HEAD WITH THIS METHOD

# In[15]:


churn2.columns


# In[16]:


churn2_col=['rownumber', 'hascrcard', 'isactivemember', 'estimatedsalary','exited']


# In[17]:


churn2.columns=churn2_col


# In[18]:


churn2.head(2)


# In[19]:


churn1_cols=['rownumber','customerid','surname','creditscore','geography','gender','age','tenure','balance','numofproducts']


# In[20]:


churn1.columns=churn1_cols


# In[21]:


churn1.columns


# In[22]:


churn1.head()


# In[23]:


churn1=pd.read_csv('C:/Users/HP/Downloads/churn_modelling1.csv',names=churn1_cols,header=0)


# In[24]:


churn1.head()


# ### ADD THE TWO DATA SET INTO A NEW VARIABLE NAME CHURN

# ### MERGE FUNCTION

# In[25]:


churn=pd.merge(churn1,churn2,on='rownumber',how='left')


# ## START TO STUDY THE DATA FOR BETTER UNDERSTANDING

# In[ ]:





# In[26]:


#TO SHOW THE 5 FIRST ROWS OF THE DATASET
churn.head()


# In[27]:


churn


# In[28]:


# TO SHOW THE LAST 5 ROWS OF THE DATASET
churn.tail()


# In[29]:


churn.index


# In[30]:


churn.sample


# In[31]:


churn.sample(frac=0.5)


# In[33]:


churn.sample(7)


# In[34]:


churn.sample(7,random_state=0)


# In[35]:


churn


# In[518]:


#LETS VISUALIZE THE DISTRIBUTION OF ANY COLUMN JUST TO SEE HOW THE COLUMNS ARE SPREAD
churn['creditscore'].sample(frac=0.001).plot.bar()


# In[519]:


churn.columns


# In[520]:


list(churn.columns)


# In[521]:


churn.size


# In[522]:


churn.shape


# In[523]:


print('number of columns',churn.shape[1])


# In[524]:


print('number of rows',churn.shape[0])


# In[43]:


churn.dtypes


# ### lets seperate objects  from numerical variables

# In[525]:


churn_catg=churn.select_dtypes('object')


# In[45]:


churn['geography'].value_counts().plot.bar()


# In[46]:


churn_cont=churn.select_dtypes('number')


# In[47]:


churn_catg


# In[48]:


churn_cont


# In[526]:


#UNIQUE VALUES SHOWS THE TYPE OF VALUE IN EACH COLUMN
churn['estimatedsalary'].nunique()


# In[527]:


churn['estimatedsalary'].unique


# In[51]:


churn['estimatedsalary'].unique()[:20]


# ## CLEANING THE DATA

# ### check if the data vave duplicates

# In[529]:


churn.duplicated()


# In[53]:


churn.duplicated().any()


# In[54]:


churn_dup=churn.duplicated().any()


# In[55]:


print(churn_dup)


# In[56]:




churn[churn.duplicated()]


# In[57]:


churn.drop_duplicates(inplace=True)


# In[58]:


churn_dup=churn.duplicated().any()


# In[59]:


print(churn_dup)


# In[60]:


churn.info()


# In[61]:


churn.count()


# ### check for missing values

# 

# In[62]:


churn.isnull().sum()


# In[63]:


churn.isnull().sum().plot.bar()


# In[64]:


churn.isnull().sum().sum()


# In[65]:


churn_missing=churn[churn.isnull().any(1)]


# In[66]:


churn_missing


# In[67]:


churn.notnull()


# In[68]:


churn_catg.isnull().sum()


# In[69]:


churn_catg.isnull().sum().plot.bar()


# In[70]:


churn_cont.isnull().sum()


# In[71]:


churn_cont.isnull().sum().plot.bar()


# ### lets calculate the amount of missing values in  percentage of the entire dataset

# In[72]:


(len(churn))


# In[73]:


(len(churn))*100


# In[74]:


churn_percentage=churn.isnull().sum()/(len(churn))*100


# In[75]:



churn_percentage


# In[76]:


(churn_percentage)*round(4)


# ### VISUALIZING MISSING VALUES

# In[77]:


import missingno as msn
msn.matrix(churn)


# In[78]:


msn.bar(churn)


# In[79]:


msn.dendrogram(churn)


# In[80]:


msn.heatmap(churn)


# In[81]:


churn_nullvalue = pd.DataFrame((churn.isnull().sum())*100/churn.shape[0]).reset_index()
churn_nullvalue.columns = ['Column Name', 'Null Values Percentage']
fig = plt.figure(figsize=(18,6))
ax = sns.pointplot(x="Column Name",y="Null Values Percentage",data=churn_nullvalue,color='green')
plt.xticks(rotation =90,fontsize =8)
ax.axhline(40, ls='--',color='red')
plt.title("Percentage of Missing values in application data")
plt.ylabel("Null Values PERCENTAGE")
plt.xlabel("COLUMNS")
plt.show()


# In[82]:


sns.heatmap(churn.isnull())


# In[83]:


churn.isnull().sum()


# In[84]:


churn=churn.drop(labels=['customerid'],axis=1)


# In[85]:


churn.isnull().sum()


# In[86]:


churn=churn.drop(labels=['rownumber','hascrcard'],axis=1)


# In[87]:


churn.isnull().sum()


# In[88]:


churn.columns


# In[89]:


len(churn.columns)


# In[90]:


churn_cont.isnull().sum().plot.bar()


# ### INPUTATION

# In[91]:


churn_cont


# In[92]:


churn.isnull().sum()


# In[93]:


churn.isnull().sum().sum()


# In[94]:


mean_churn1=churn['estimatedsalary'].mean()


# In[95]:


mean_churn1


# In[96]:


churn['estimatedsalary'].fillna(mean_churn1,inplace=True)


# In[97]:


churn.isnull().sum()


# In[98]:


churn.isnull().sum().sum()


# In[99]:


churn['isactivemember'].unique()


# In[100]:


churn.isnull().sum()


# In[101]:


churn.isnull().sum().sum()


# In[102]:


churn_cont.head(2)


# In[103]:


#drop all categorical or object or string missing values
churn.loc[:,['surname','geography',]].dropna()


# In[104]:


churn.isnull().sum()


# In[105]:


churn.gender.unique()


# In[106]:


len(churn.gender.unique())


# In[107]:


churn_mod=churn.gender.value_counts()


# In[108]:


churn_mod


# In[109]:


#mode is male
churn['gender'].fillna('Male',inplace=True)


# In[110]:


churn['gender'].fillna('churn_mod',inplace=True)


# In[111]:


churn.isnull().sum()


# In[112]:


churn.isnull().sum().sum()


# In[113]:


churn.dropna(inplace=True)


# In[114]:


churn.isnull().sum()


# In[115]:


churn.isnull().sum().sum()


# ## VISUALIZING THE DATA AFTER  TAKING CARE OF THE MISSING VALUES

# In[116]:


#check new visuals
msn.matrix(churn)


# In[117]:


msn.bar(churn)


# In[118]:


msn.dendrogram(churn)


# In[119]:


msn.heatmap(churn)


# In[120]:


churn_nullvalue = pd.DataFrame((churn.isnull().sum())*100/churn.shape[0]).reset_index()
churn_nullvalue.columns = ['Column Name', 'Null Values Percentage']
fig = plt.figure(figsize=(18,6))
ax = sns.pointplot(x="Column Name",y="Null Values Percentage",data=churn_nullvalue,color='green')
plt.xticks(rotation =90,fontsize =8)
ax.axhline(40, ls='--',color='red')
plt.title("Percentage of Missing values in application data")
plt.ylabel("Null Values PERCENTAGE")
plt.xlabel("COLUMNS")
plt.show()


# In[121]:


sns.heatmap(churn.isnull())


# ## OUTLIERS

# In[122]:


churn.head(2)


# In[123]:


churn.describe()


# In[124]:


sns.distplot(churn['balance'],kde=False,hist=True,bins=12)
plt.title('balance distribution',size=16)
plt.ylabel('count')


# In[125]:


churn.boxplot()


# In[126]:


sns.distplot(churn['balance'])


# In[127]:


churn['estimatedsalary'].plot(kind='box',figsize=(3,4),patch_artist=True)


# In[128]:


sns.boxplot(x='gender',y='age',data=churn)


# ### DETECT OUTLIERS

# ### Z-SCORE METHOD

# In[129]:


churn['balance'].plot(kind='box',figsize=(3,4),patch_artist=True)


# In[130]:


churn['age'].unique()


# In[131]:


sns.boxplot(y='age',data=churn)


# In[132]:


churn['age'].mean()


# In[133]:


churn1=churn[churn['age']<=70]


# In[134]:


sns.boxplot(y='age',data=churn1)


# In[135]:


churn2=churn[churn['age']<=58]


# In[136]:


sns.boxplot(y='age',data=churn2)


# In[ ]:





# In[137]:


sns.boxplot(y='balance',data=churn)


# In[138]:


churn2=churn


# In[139]:


churn['balance'].mean()


# In[140]:


churn.shape


# In[141]:


churn.boxplot()


# In[142]:


churn.hist()


# ## DETECT AND REMOVE OUTLIERS

# ### Data points, that falls outside of 1.5 times of the interquartile range above the 3rd or below the 1st quartile
# ### is an outlier.
# 

# In[143]:


churn.describe()


# In[144]:


churn.shape


# ### BOXPLOT

# In[145]:


#DEFINE A FUNCTION CALLED plot_box 
#and there will be two arguments call it df and ft
#and plot the boxplot with the boxplot function in pandas
def plot_boxplot(churn1,ft):
    churn.boxplot(column=[ft])
    plt.grid(False)
    plt.show()


# In[146]:


#lets call it
plot_boxplot(churn,'creditscore')


# In[147]:


sns.boxplot(y='age',data=churn1)


# In[148]:


sns.boxplot(y='creditscore',data=churn1)


# In[149]:


plot_boxplot(churn,'tenure')


# In[150]:


plot_boxplot(churn,'balance')


# In[151]:


plot_boxplot(churn,'numofproducts')


# In[152]:


churn1=churn[churn['numofproducts']<=3.5]


# In[153]:


plot_boxplot(churn1,'numofproducts')


# In[154]:


plot_boxplot(churn,'estimatedsalary')


# ### REMOVE OUTLIERS

# In[155]:


#FIRST WE WILL EXTRAT ALL OUTLIERS IN ALL COLUMN 
# SECONDLY WE USE THE INDEXES OF THE OUTLIERS TO REMOVE THEM FROM THE DATAFRAME  BY CREATING A LIST 
#DEFINE A FUNCTION CALLED OULIERS
# IQR =Q3 -  Q1 
# +/- 1.5 * IQR
# SET THE UPPER AND LOWER BOUNDRIES


# In[156]:


Q1=churn.creditscore.quantile(0.25)
Q3=churn.creditscore.quantile(0.75)


# In[157]:


Q1,Q3


# In[158]:


plt.hist(churn['creditscore'],)
plt.title('distribution of total creditscore')
plt.show()


# In[159]:


sns.distplot(churn['creditscore'])


# In[160]:


IQR=Q3-Q1 


# In[161]:


IQR


# In[162]:


lower_limit = Q1-(1.5*IQR)


# In[163]:


lower_limit


# In[164]:


upper_limit=Q3+(1.5*IQR)


# In[165]:


upper_limit


# In[166]:


lower_limit, upper_limit


# In[167]:


churn[(churn.creditscore<lower_limit)|(churn.creditscore>upper_limit)]


# In[168]:


(churn['creditscore']<916.5)


# In[169]:


(churn['creditscore']>384.5)


# In[170]:


churn2=churn[(churn['creditscore']<916.5)&(churn['creditscore']>384.5)]


# In[171]:


churn2['creditscore'].plot(kind='box', figsize=(3,4), patch_artist=True)


# In[172]:


churn2.shape


# In[173]:


plt.hist(churn2['creditscore'])
plt.title('distribution of total creditscore')
plt.show()


# In[174]:


sns.distplot(churn2['creditscore'])


# In[175]:


churn2=churn


# In[176]:


#cross check by visualizing the box plot
plot_boxplot(churn,'creditscore')


# In[177]:


plot_boxplot(churn,'age')


# In[178]:


#SAVE THE NEW DATA FRAME IN PANDAS
churn.head(2)


# In[179]:


churn.shape


# In[180]:


churn.describe()


# In[181]:


churn.boxplot()


# In[182]:


sns.pairplot(churn)


# ### EXPLORATORY DATA ANALYSIS

# In[183]:


churn.corr()


# In[184]:


plt.figure(figsize=(20,20))
sns.heatmap(churn.corr(), vmin=-1, cmap="plasma_r", annot=True)
#same thing can be seen from the correlation as well


# In[185]:


churn.head(2)


# ### form the corrolation heatmap it shows exited which is the target variable is highly corrolated with the following independent variable, estimated salary , balance and age.
# 

# In[ ]:





# ## EXPLORATORY DATA ANALYSIS

# ###  QUERRYING THE DATASET

# ##### Q1,WHAT IS THE MEAN, MIN , AND MAX  VALUE OF   estimatedsalary

# #####  GROUPBY

# In[186]:


churn.groupby('estimatedsalary').mean()


# In[ ]:





# In[187]:


churn.groupby('creditscore').min()


# #### Q2, FIND ALL INSTANCES WHEN CREDITSCORE IS GREATER THAN OR EQUALL TO 600 OR GEOGRAPHY IS FRANCE

# In[ ]:





# In[188]:


churn[(churn['creditscore']>=600)|(churn['geography']=='france')]


# In[ ]:





# In[189]:


list[churn[(churn['creditscore']<600)|(churn['geography']=='france')]]


# ####  Q3 SHOW ALL RECORDS WHERE  CREDITSTORE IS 502.0 OR 518.0

# #####  IS IN

# In[190]:


churn['creditscore'].isin(['502.0','518.0'])


# #### QUESTION 4 FIND  TOTAL NUMBER OF RECORDS HAVING CREDITSCORE OF 502 TO 518 INCLUSIVE USING BETWEEN METHOD

# In[191]:


[churn['creditscore']>=502]


# In[192]:


[churn['creditscore']<=518]


# In[193]:


sum(churn['creditscore'].between(502,518))


# In[194]:


churn.head(2)


# ## geography

# ### Q5 how many people have spain as thier geographical location

# In[195]:


churn['geography']


# In[196]:


churn['geography'].str.contains('spain',case=False)


# In[197]:


churn[churn['geography'].str.contains('spain',case=False)]


# In[198]:


len(churn['geography'].str.contains('spain',case=False))


# ### Q6 TOTAL NUMBER OF  RECORDS IN GEOGRAPHY

# In[199]:


churn['geography'].nunique()


# In[200]:


churn.head(2)


# ### Q7 WHICH GEOGRAPHY HAS THE HIGEST CREDITSCORE   AND WHICH GEOGRAPHY  HAVE THE HIGHEST  AVERAGE RATING

# In[201]:


churn.groupby('geography')['creditscore'].max()


# In[202]:


churn.groupby('geography')['creditscore'].mean()


# In[203]:


churn.groupby('geography')['creditscore'].mean().sort_values(ascending=False)


# In[204]:


churn.head(2)


# ### Q8  TOTAL NUMBER OF GEOGRAPHY  HAVING 2 TENURE

# In[205]:


churn.columns


# In[206]:


churn['tenure']


# In[207]:


churn['tenure']==2


# In[208]:


churn[churn['tenure']==2]


# In[209]:


len(churn[churn['tenure']==2])


# ### GENDER

# ### Q9 FIND TOTAL NUMBER OF MALE AND FEMALE

# In[210]:


churn.head(2)


# In[211]:


churn['gender'].value_counts()


# ###  Q10 WHICH GENDER HAVE THE HIGEST BALANCE

# In[212]:


churn['balance'].max()


# In[213]:


churn[churn['balance'].max()==churn['balance']]['gender']


# ### QUESTION11, DISPLAY TOP 5 NAMES HAVING MAXIMUM NUMBER OF SALARY

# In[214]:


churn.head(2)


# In[215]:


churn_index=churn['estimatedsalary'].sort_values(ascending=False).head().index


# In[216]:


churn.iloc[churn_index]


# ### QUESTION12 WHAT IS THE RECORDS OF THE SURNAME WATTS

# In[217]:


churn.head(2)


# In[218]:


churn['surname']


# In[219]:


churn['surname'].isin(['Watts'])


# In[220]:


churn[churn['surname'].isin(['Watts'])]


# In[221]:


# WE CAN ALSO USE STR CONTAINS
churn[churn['surname'].str.contains('Watts')]


# ###  AGE

# ### Q13 DISPLAY TOP 5 OLDEST GENDER

# In[222]:


churn['age'].unique


# In[223]:


churn['age'].sort_values(ascending=False)


# In[224]:


churn['age'].sort_values(ascending=False).head()


# In[225]:


index=churn['age'].sort_values(ascending=False).head().index


# In[226]:


churn.iloc[index]


# ### Q14 GIVE ALL THE NAMES THAT HAVE AGE 50

# In[227]:


churn['age']


# In[228]:


churn[churn['age']==50.0]


# In[229]:


len(churn[churn['age']==50.0])


# ### Q15 SHOW ALL THE  RECORDS THAT HAVE  BALANCE ABOVE 83807.86

# ### BALANCE

# In[230]:


churn.head(2)


# In[231]:


churn['balance']>=83807.86


# In[232]:


churn[churn['balance']>=83807.86]


# In[233]:


churn['balance'].unique()


# #### QUESTION 16  SHOW  ONLY THE BALANCE     OF  ALL GENDER FEMALE  WHO LIVES IN FRANCE ONLY

# In[234]:


(churn['gender']=='Female')


# In[235]:


(churn['geography']=='France')


# In[236]:


(churn['gender']=='Female')&(churn['geography']=='France')


# In[237]:


churn[(churn['gender']=='Female')&(churn['geography']=='France')]['balance']


# ### QUESTION 17 SHOW ALL RECORDS WERE  GENDER IS MALE AND GEOGRAPHY IS SPAIN  OR BALANCE IS 159660.8

# In[238]:


churn.head(2)


# In[239]:


churn['gender'].unique()


# In[240]:


(churn['gender']=='Male')


# In[241]:


(churn['geography']=='Spain')


# In[242]:


(churn['gender']=='Male')&(churn['geography']=='Spain')


# In[243]:


churn[(churn['gender']=='Male')&(churn['geography']=='Spain')]


# In[244]:


churn[(churn['gender']=='Male')&(churn['geography']=='Spain')|(churn['balance']=='159660.8')]


# In[ ]:





# In[245]:


churn[(churn['gender']=='Male')&(churn['geography']=='Spain')|(churn['surname']=='Lorenzo')]


# ### NUMBEROFPRODUCTS

# ### QUESTION 18 WHO HAS THE HIGHEST NUMBER OF PRODUCTS

# In[246]:


churn.head(2)


# In[247]:


churn.numofproducts.value_counts()


# In[248]:


churn.numofproducts.unique()


# In[249]:


churn.head(2)


# ### QUESTION 19 , WHICH  GENDER HAS THE HIGHEST AVERAGE  BALANCE

# In[250]:


churn.columns


# In[251]:


churn.groupby('gender')['balance'].mean().sort_values(ascending=False)


# In[252]:


sns.barplot(x='gender',y='balance',data=churn)
plt.title('gender by balance')
plt.show()


# ### Q 20,DISPLAY TO 10 BALANCE  AND GEOGRAPHY

# In[253]:


churn.columns


# In[254]:


churn_top10=churn.nlargest(10,('balance'))[['geography']]


# In[255]:


churn_top10


# In[256]:


churn_top10=churn.nlargest(10,('balance'))[['geography']].set_index('geography')


# In[257]:


churn_top10


# In[258]:


churn_top10=churn.nsmallest(10,('balance'))[['geography']]


# In[259]:


churn_top10


# In[260]:


churn_top10=churn.nlargest(10,('balance'))[['geography','gender']]


# In[261]:


churn_top10


# In[ ]:





# ### Q21 , DISPLAY TOTAL NUMBER OF SALARY IN THE DATASET

# In[262]:


churn['estimatedsalary'].value_counts()


# In[263]:


sns.countplot(x='estimatedsalary',data=churn)
plt.title('no of estimatedsalary')
plt.show()


# ### QUESTION 22, FIND THE MOST POPULAR GEOGRAPHY WITH HIGHEST CREDITSCORE

# In[264]:


churn.columns


# In[265]:


churn['creditscore'].max()


# In[266]:


churn['creditscore']


# In[267]:


[churn['creditscore'].max()==churn['creditscore']]


# In[268]:


churn[churn['creditscore'].max()==churn['creditscore']]


# In[269]:


churn[churn['creditscore'].max()==churn['creditscore']]['geography']


# ### Q 23, DOES SALARY AFFECT BALANCE

# In[270]:


churn.columns


# In[271]:


sns.scatterplot(x='estimatedsalary',y='balance',data=churn)


# ### Q24,  CLASSIFY CREDITSCORE BASED ON EXITED

# In[272]:


churn


# In[ ]:





# In[273]:


churn.head(2)


# ### VISUALYZING AND EXPLORATORY DATA ANALYSIS (EDA)

# #### GEOGRAPHY

# In[274]:


churn.geography.unique()


# In[275]:


sns.set(style='whitegrid')
plt.figure(figsize=(14, 7))
labels=['France', 'Spain','Germany'],
plt.pie(churn['geography'].value_counts(),labels=['France', 'Spain','Germany'],explode=[0.1,0.1,0.1],
        autopct='%1.2f%%',colors=['#E37383','#FFC0CB'], startangle=90)
plt.title('geography')
plt.axis('equal')
plt.show()


# ### GENDER

# In[276]:


plt.figure(figsize=(8,5))
sns.countplot('gender', data = churn, color='#00ddff', saturation=0.9)


# In[277]:


plt.figure(figsize=(20,20))
sns.heatmap(churn.corr(), vmin=-1, cmap="plasma_r", annot=True)
#same thing can be seen from the correlation as well


# In[278]:


churn.head(2)


# In[279]:


sns.scatterplot(x='age',y='exited',data=churn)


# In[280]:


sns.scatterplot(x='age',y='exited',data=churn,hue='gender',palette='RdYlBu')


# In[281]:


sns.barplot(x='gender',y='exited',data=churn,palette='RdYlBu')


# In[282]:


sns.histplot(churn['tenure'],kde=True,bins=15)


# In[ ]:





# In[283]:


sns.boxplot(x='geography',y='exited',data=churn,hue='gender',palette='YlGnBu')


# In[284]:


sns.histplot(churn['estimatedsalary'],kde=True,bins=15)


# In[285]:


sns.barplot(x='gender',y='estimatedsalary',data=churn,palette='RdYlBu')


# In[286]:


sns.boxplot(x='geography',y='estimatedsalary',data=churn,hue='gender',palette='YlGnBu')


# In[287]:


sns.histplot(churn['balance'],kde=True,bins=15)


# In[288]:


sns.barplot(x='gender',y='balance',data=churn,palette='RdYlBu')


# In[289]:


sns.boxplot(x='geography',y='balance',data=churn,hue='gender',palette='YlGnBu')


# In[290]:


sns.histplot(churn['creditscore'],kde=True,bins=15)


# In[291]:


sns.barplot(x='gender',y='creditscore',data=churn,palette='RdYlBu')


# In[292]:


sns.boxplot(x='geography',y='creditscore',data=churn,hue='gender',palette='YlGnBu')


# In[293]:


sns.stripplot(x='geography',y='creditscore',data=churn,hue='gender',dodge=True ,palette='YlGnBu')


# In[294]:


sns.stripplot(x='geography',y='exited',data=churn,hue='gender',dodge=True ,palette='YlGnBu')


# In[ ]:





# In[295]:


churn.head(2)


# In[296]:


sns.jointplot(x='exited',y='balance',data=churn,kind='reg')


# In[297]:


sns.jointplot(x='exited',y='estimatedsalary',data=churn)


# In[298]:


sns.jointplot(x='creditscore',y='estimatedsalary',data=churn,)


# In[299]:


sns.jointplot(x='creditscore',y='estimatedsalary',data=churn,kind='kde')


# In[300]:


sns.jointplot(x='creditscore',y='estimatedsalary',data=churn,kind='kde',shade=True)


# In[301]:


sns.jointplot(x='creditscore',y='estimatedsalary',data=churn,kind='kde',shade=True,cmap='YlGnBu')


# In[302]:


sns.jointplot(x='creditscore',y='estimatedsalary',data=churn,kind='hex',cmap='YlGnBu')


# In[303]:


sns.countplot(x='exited',hue='gender',data=churn)


# In[304]:


sns.countplot(x='exited',hue='geography',data=churn)


# In[305]:


churn['age'].plot.hist()


# In[306]:


churn['balance'].plot.hist(bins=20,figsize=(10,5))


# In[307]:


churn['creditscore'].plot.hist(bins=20,figsize=(10,5))


# In[308]:


churn['estimatedsalary'].plot.hist(bins=20,figsize=(10,5))


# In[309]:


churn.head(2)


# In[310]:


churn['exited'].unique


# In[311]:


#visualization of this data
cols=['creditscore','geography','gender','age','tenure','balance','numofproducts','isactivemember','estimatedsalary','exited']
n_rows=2
n_cols=3
#the subplot grid and figsize of the graph
fig,  axs =plt.subplots(n_rows,n_cols,figsize=(n_cols*3.2,n_rows*3.2))
for r in range(0,n_rows):
    for c in range (0,n_cols):
        i = r*n_cols+c  #index to go through the number of columns
        ax= axs[r][c]   #show were to position each subplot
        sns.countplot(churn[cols[i]],hue=churn['exited'],ax=ax)
        ax.set_title(cols[i])
        ax.legend(title='exited',loc='upper right')
        plt.tight_layout


# In[312]:


churn.pivot_table('exited',index='gender',columns='geography').plot()


# In[313]:


f, ax =plt.subplots(1,2,figsize= (18,8))
churn['exited'].value_counts().plot.pie(explode=[0,0.1,],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('exited')
ax[0].set_ylabel('')
sns.countplot('exited',data=churn,ax=ax[1])
ax[1].set_title('exited')
plt.show()


# In[314]:


churn.head(2)


# In[315]:


churn_gender = churn.groupby(['gender'])[['creditscore', 'balance', 'estimatedsalary']].mean()


# In[316]:


colors_4 = ['magenta','yellow','green','red']
colors_3 =['magenta','yellow','green']


# In[317]:


churn_gender.plot.pie(subplots=True, figsize=(20,10), labels=churn_gender.index, autopct='%1.1f%%',colors=colors_4)
plt.show()


# In[318]:


churn_geography = churn.groupby(['geography'])[['creditscore', 'balance', 'estimatedsalary']].mean()


# In[319]:


churn_geography.plot.pie(subplots=True, figsize=(20,10), labels=churn_geography.index, autopct='%1.1f%%',colors=colors_4)
plt.show()


# In[320]:


sns.factorplot('geography','exited',hue='gender',data=churn)


# In[321]:


f, ax = plt.subplots(1,2,figsize=(18,8,))
sns.violinplot('geography','age', hue='exited',data=churn,ax=ax[0])
ax[0].set_title('geography and age vs exited')
ax[0].set_yticks(range(0,110,10))
sns.violinplot('gender','age',hue='exited',data=churn,ax=ax[1])
ax[1].set_title('gender and age vs exited')
ax[1].set_yticks(range(0,110,10))
plt.show()


# In[322]:


f, ax=plt.subplots(1,2,figsize=(12,10))
churn[churn['exited']==0].age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')
ax[0].set_title('exited=0')
x1=list(range(0,85,5))
ax[0].set_xticks(x1)
churn[churn['exited']==1].age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')
ax[1].set_title('exited=1')
x2=list(range(0,85,5))
ax[1].set_xticks(x2)
plt.show()


# In[323]:


plt.figure(figsize=(30,30))
sns.set_context("talk",font_scale=1)
sns.set_palette("pastel")
ax = sns.countplot(y="age", hue="gender", data=churn)
ax.legend(loc='upper right',frameon=True)
plt.title('GENDER vs AGE', fontsize=18, fontweight='bold')
ax.set(xlabel='COUNT OF GENDER',ylabel='AGE')
plt.show()


# In[324]:


plt.figure(figsize=(6,6))
sns.set_context("talk",font_scale=1)
sns.set_palette("pastel")
ax = sns.countplot(y="gender", hue="exited", data=churn)
ax.legend(loc='upper right',frameon=True)
plt.title('exited vs gender', fontsize=8, fontweight='bold')
ax.set(xlabel='COUNT OF exited ',ylabel='gender')
plt.show()


# In[325]:


plt.figure(figsize=(8,8))
plt.title('Correlation Analysis',color='Red',fontsize=20,pad=40)

corr = churn.corr()
mask = np.triu(np.ones_like(corr,dtype = bool))
sns.heatmap(churn.corr(),mask=mask,annot=True,linewidths=.5);
plt.xticks(rotation=60)
plt.yticks(rotation = 60)
plt.show()


# In[326]:


colors = ['#E94B3C','#2D2926']

exited = churn[churn['exited'] == 1].describe().T
not_exited = churn[churn['exited'] == 0].describe().T

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(exited[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('exited Customers');

plt.subplot(1,2,2)
sns.heatmap(not_exited[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Not_exited Customers');

fig.tight_layout(pad = 0)


# In[327]:


l = list(churn['exited'].value_counts())
circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]

fig = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))
plt.subplot(1,2,1)
plt.pie(circle,labels = ['Not-exited Customer','exited Customer'],autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors,
       wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})
plt.title('exited - Not-exited %');

plt.subplot(1,2,2)
ax = sns.countplot('exited',data = churn,palette = colors,edgecolor = 'black')
for rect in ax.patches:
    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)

    
plt.title('Number of Exited - Not-Exited Customers');
plt.show()


# ### DATA SCIENCE 

# In[328]:


churn.head(5)


# ### DATA PREPROCESSING

# ###  converting categorical variable to numerical cvariable we can 
# ### use 2 methods ENCODING AND MAPP METHODS

# #### MAP METHOD OF CONVERTING CATEGORICAL TO NUMERICAL 

# In[329]:


churn.head()


# In[330]:


churn=churn.drop(churn.columns[[0]],axis=1)


# In[331]:


churn.dtypes


# In[332]:


churn_catg


# In[333]:


churn_cont.head(2)


# In[334]:


churn1


# In[335]:


churn.head()


# #### REPLACE METHOD

# In[336]:


#geography1={'geography':{'France':0,'Spain':1,'Germany':2}}


# In[337]:


#churn=churn.replace(geography1)


# In[338]:


churn.head()


# ### LABEL ENCODER METHOD

# In[339]:


from sklearn.preprocessing import LabelEncoder


# In[340]:


le=LabelEncoder()


# In[341]:


churn['geography1']=le.fit_transform(churn.geography)


# In[342]:


churn.head()


# ### dummies variable methiod

# In[343]:


churn=pd.get_dummies(churn,drop_first=True)


# In[344]:


churn.head()


# In[345]:


#churn=pd.get_dummies(churn,columns=['gender'],prefix='gender',drop_first=True)


# ### INBALANCE DATASET 

# ### UNDERSAMPLING  AND OVERSAMPLING METHODS

# In[346]:


churn.head(2)


# In[347]:


churn['exited'].value_counts()


# In[348]:


sns.countplot(churn['exited'])


# ### store feature matrix in X  and response target in vector y

# In[349]:


X= churn.drop('exited',axis=1)


# In[350]:


y=churn['exited']


# In[351]:


X


# In[352]:


y


# ### split dataset into training set and test set

# In[353]:


from sklearn.model_selection import train_test_split


# In[354]:


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42,stratify=y)


# In[355]:


X_train


# In[356]:


y_train


# ## MACHINE LEARNING ALGRITHMS

# ### LOGISTIC  REGRESSION

# In[357]:


churn.head()


# In[358]:


X


# # FEAUTURE SCALING

# In[359]:


from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler


# In[360]:


sc=StandardScaler()


# In[361]:


X_train=sc.fit_transform(X_train)


# In[362]:


X_test=sc.fit_transform(X_test)


# In[363]:


X_train


# In[364]:


from sklearn.linear_model import LogisticRegression


# In[365]:


log=LogisticRegression()


# In[366]:


log.fit(X_train,y_train)


# In[367]:


y_pred1=log.predict(X_test)


# In[368]:


from sklearn.metrics import accuracy_score


# In[369]:


accuracy_score(y_test,y_pred1)


# In[370]:


from sklearn.metrics import precision_score,recall_score,f1_score


# In[371]:


precision_score(y_test,y_pred1)


# In[372]:


recall_score(y_test,y_pred1)


# In[373]:


f1_score(y_test,y_pred1)


# ### EVALUATE IMBALANCE DATA SET FIRST BY UNDERSAMPLING

# In[374]:


normal=churn[churn['exited']==0]
fraud=churn[churn['exited']==1]


# In[375]:


normal.shape


# In[376]:


fraud.shape


# In[377]:


normal_sample=normal.sample(n=2055)


# In[378]:


normal_sample.shape


# In[379]:


new_churn=pd.concat([normal_sample,fraud])


# In[380]:


new_churn['exited'].value_counts()


# In[381]:


new_churn.head()


# In[382]:


X=new_churn.drop('exited',axis=1)


# In[383]:


y=new_churn['exited']


# In[384]:


from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)


# In[385]:


log=LogisticRegression()
log.fit(X_train,y_train)


# In[386]:


y_pred1=log.predict(X_test)


# In[387]:


accuracy_score(y_test,y_pred1)


# In[388]:


from sklearn.metrics import precision_score,recall_score,f1_score


# In[389]:


precision_score(y_test,y_pred1)


# In[390]:


recall_score(y_test,y_pred1)


# In[391]:


f1_score(y_test,y_pred1)


# ### DECISION TREE CLASSIFIER

# In[392]:


from sklearn.tree import DecisionTreeClassifier


# In[393]:


dt=DecisionTreeClassifier()


# In[394]:


dt.fit(X_train,y_train)


# In[395]:


y_pred2=dt.predict(X_test)


# In[396]:


accuracy_score(y_test,y_pred2)


# In[397]:



f1_score(y_test,y_pred2)


# In[ ]:





# In[ ]:





# In[398]:


precision_score(y_test,y_pred2)


# In[399]:


recall_score(y_test,y_pred2)


# ### RANDOM FOREST CLASSIFIER

# In[400]:


from sklearn.ensemble import RandomForestClassifier


# In[401]:


rf=RandomForestClassifier()


# In[402]:


rf.fit(X_train,y_train)


# In[403]:


y_pred3=rf.predict(X_test)


# In[404]:


accuracy_score(y_test,y_pred3)


# In[405]:


f1_score(y_test,y_pred3)


# In[406]:


precision_score(y_test,y_pred3)


# In[407]:


recall_score(y_test,y_pred3)


# ### visualizing the results after after undersampling

# In[408]:


final_data=pd.DataFrame({'Models':['log','dt','rf'],
             'ACC':[accuracy_score(y_test,y_pred1)*100,
                                               
                                                    
                    accuracy_score(y_test,y_pred2)*100,
                    accuracy_score(y_test,y_pred3)*100]}) 


# In[409]:


final_data


# In[410]:


sns.barplot(final_data['Models'],final_data['ACC'])


# ### OVERSAMPLING USING SMOTE

# ### synthetic minority over sampling

# In[411]:


churn.head()


# In[412]:


churn.shape


# In[413]:


churn['exited'].value_counts()


# In[414]:


sns.countplot(churn['exited'])


# In[415]:


X= churn.drop('exited',axis=1)


# In[416]:


X


# In[417]:


y=churn['exited']


# In[418]:


y


# In[419]:


from sklearn.model_selection import train_test_split


# In[420]:


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42,stratify=y)


# In[421]:


X_train


# In[422]:


y_train


# In[423]:


from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler


# In[424]:


minmax = MinMaxScaler()


# In[425]:


X_train=minmax.fit_transform(X_train)


# In[426]:


X_test=minmax.fit_transform(X_test)


# In[427]:


X_train


# In[428]:


from imblearn.over_sampling  import SMOTE


# In[429]:


SMOTE().fit_resample(X,y)


# In[430]:


X_res,y_res=SMOTE().fit_resample(X,y)


# In[431]:


y_res.value_counts()


# In[432]:


X_train,X_test,y_train,y_test=train_test_split(X_res,y_res,test_size=0.20,random_state=42,)


# In[433]:


from sklearn.linear_model import LogisticRegression


# In[434]:


log=LogisticRegression()


# In[435]:


log.fit(X_train,y_train)


# In[436]:


y_pred1=log.predict(X_test)


# In[437]:


from sklearn.metrics import accuracy_score


# In[438]:


accuracy_score(y_test,y_pred1)


# In[439]:


from sklearn.metrics import precision_score,recall_score,f1_score


# In[440]:


precision_score(y_test,y_pred1)


# In[441]:


recall_score(y_test,y_pred1)


# In[442]:


f1_score(y_test,y_pred1)


# ## SVC

# In[443]:


from sklearn import svm


# In[444]:


svm=svm.SVC()


# In[445]:


svm.fit(X_train,y_train)


# In[446]:


y_pred2=svm.predict(X_test)


# In[447]:


accuracy_score(y_test,y_pred2)


# In[448]:


precision_score(y_test,y_pred2)


# In[449]:


f1_score(y_test,y_pred2)


# ### KNeighbors Classifier

# In[450]:


from sklearn.neighbors import KNeighborsClassifier


# In[451]:


knn=KNeighborsClassifier()


# In[452]:


knn.fit(X_train,y_train)


# In[453]:


y_pred3=knn.predict(X_test)


# In[454]:


accuracy_score(y_test,y_pred3)


# In[455]:


precision_score(y_test,y_pred3)


# In[456]:


recall_score(y_test,y_pred3)


# In[457]:


f1_score(y_test,y_pred3)


# ## Decision Tree Classifier

# In[458]:


from sklearn.tree import DecisionTreeClassifier


# In[459]:


dt=DecisionTreeClassifier()


# In[460]:


dt.fit(X_train,y_train)


# In[461]:


y_pred4=dt.predict(X_test)


# In[462]:


accuracy_score(y_test,y_pred4)


# In[463]:


precision_score(y_test,y_pred4)


# In[464]:


recall_score(y_test,y_pred4)


# In[465]:


f1_score(y_test,y_pred4)


# ###  Random Forest Classifier

# In[466]:


from sklearn.ensemble import RandomForestClassifier


# In[467]:


rf=RandomForestClassifier()


# In[468]:


rf.fit(X_train,y_train)


# In[469]:


y_pred5=rf.predict(X_test)


# In[470]:


accuracy_score(y_test,y_pred5)


# In[471]:


precision_score(y_test,y_pred5)


# In[472]:


recall_score(y_test,y_pred5)


# In[473]:


f1_score(y_test,y_pred5)


# ### Gradient Boosting Classifier

# In[474]:


from sklearn.ensemble import GradientBoostingClassifier


# In[475]:


gb=GradientBoostingClassifier()


# In[476]:


gb.fit(X_train,y_train)


# In[477]:


y_pred6=gb.predict(X_test)


# In[478]:


accuracy_score(y_test,y_pred6)


# In[479]:


precision_score(y_test,y_pred6)


# In[480]:


recall_score(y_test,y_pred6)


# In[481]:


f1_score(y_test,y_pred6)


# ### SAVE THE MODEL

# In[482]:


final_data1=pd.DataFrame({'Models':['log','svc','knn','dt','rf','gb'],
             'ACC':[accuracy_score(y_test,y_pred1)*100,
                    accuracy_score(y_test,y_pred2)*100,                           
                    accuracy_score(y_test,y_pred3)*100,                                
                    accuracy_score(y_test,y_pred4)*100,
                    accuracy_score(y_test,y_pred5)*100,
                    accuracy_score(y_test,y_pred6)*100]}) 


# In[483]:


final_data1


# In[484]:


sns.barplot(final_data1['Models'],final_data1['ACC'])


# In[485]:


final_data2=pd.DataFrame({'Models':['log','svc','knn','dt','rf','gb'],
             'PRE':[precision_score(y_test,y_pred1)*100,
                    precision_score(y_test,y_pred2)*100,                           
                    precision_score(y_test,y_pred3)*100,                                
                    precision_score(y_test,y_pred4)*100,
                    precision_score(y_test,y_pred5)*100,
                    precision_score(y_test,y_pred6)*100]}) 


# In[486]:


final_data2


# In[487]:


sns.barplot(final_data2['Models'],final_data2['PRE'])


# In[488]:


final_data3=pd.DataFrame({'Models':['log','svc','knn','dt','rf','gb'],
             'REC':[recall_score(y_test,y_pred1)*100,
                    recall_score(y_test,y_pred2)*100,                           
                    recall_score(y_test,y_pred3)*100,                                
                    recall_score(y_test,y_pred4)*100,
                    recall_score(y_test,y_pred5)*100,
                    recall_score(y_test,y_pred6)*100]}) 


# In[489]:


final_data3


# In[490]:


sns.barplot(final_data3['Models'],final_data3['REC'])


# In[491]:


final_data4=pd.DataFrame({'Models':['log','svc','knn','dt','rf','gb'],
             'F1':[f1_score(y_test,y_pred1)*100,
                    f1_score(y_test,y_pred2)*100,                           
                    f1_score(y_test,y_pred3)*100,                                
                    f1_score(y_test,y_pred4)*100,
                    f1_score(y_test,y_pred5)*100,
                    f1_score(y_test,y_pred6)*100]}) 


# In[492]:


final_data4


# In[493]:


sns.barplot(final_data4['Models'],final_data4['F1'])


# In[494]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import pandas as pd


# ### DATA SCIENCE DEVELOPER

# #### SAVING AND DEVELOPING THE MODEL AND DEPLOYMENT

# In[495]:


X_res=minmax.fit_transform(X_res)


# In[496]:


rf.fit(X_res,y_res)


# In[497]:


import joblib


# In[498]:


joblib.dump(rf,'churn_predict_model')


# In[499]:


model=joblib.load('churn_predict_model')


# In[500]:


churn.columns


# In[501]:


churn.head()


# In[502]:


model.predict([[619,0,42,2,0.00,1.00,1.00,101348.88,1.00,0,0]])


# ## GUI

# In[503]:


from tkinter import *


# In[504]:


import joblib


# In[505]:


from tkinter import messagebox


# In[506]:



master=Tk()





master.mainloop()


# In[507]:


#ADD TITLE TO IT
master=Tk()

master.title('churn_predict_model')



master.mainloop()


# In[508]:


#LETS SET DIMENSION OF THIS WINDOW
master=Tk()

master.title('churn_predict_model')

master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.mainloop()


# In[509]:


#LETS ADD COLOR TO THE WINDOW

master=Tk()

master.title('churn_predict_model')

master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.configure(bg='lightblue')







master.mainloop()


# In[510]:


master=Tk()
master.title('churn_predict_model')

master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.configure(bg='lightblue')


label=Label(master,text='Bank churn',font=('Arial',20,'bold'),width=10,height=1,bg='lightblue',foreground='yellow').grid(row=0,columnspan=2)

Label(master,text='creditscore').grid(row=1)
Label(master,text='gender[0-1]').grid(row=2)
Label(master,text='age').grid(row=3)
Label(master,text='tenure').grid(row=4)
Label(master,text='balance').grid(row=5)
Label(master,text='	numofproducts').grid(row=6)
Label(master,text='isactivemember').grid(row=7)
Label(master,text='estimatedsalary').grid(row=8)
Label(master,text='geography1').grid(row=9)
Label(master,text='geography_Germany').grid(row=10)
Label(master,text='geography_Spain').grid(row=11)
master.mainloop()


# In[511]:



master=Tk()
master.title('churn_predict_model')

master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.configure(bg='lightblue')


label=Label(master,text='Bank churn',font=('Arial',20,'bold'),width=10,height=1,bg='lightblue',foreground='yellow').grid(row=0,columnspan=2)

Label(master,text='creditscore').grid(row=1)
Label(master,text='gender[0-1]').grid(row=2)
Label(master,text='age').grid(row=3)
Label(master,text='tenure').grid(row=4)
Label(master,text='balance').grid(row=5)
Label(master,text='	numofproducts').grid(row=6)
Label(master,text='isactivemember').grid(row=7)
Label(master,text='estimatedsalary').grid(row=8)
Label(master,text='geography1').grid(row=9)
Label(master,text='geography_Germany').grid(row=10)
Label(master,text='geography_Spain').grid(row=11)


e1=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e2=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e3=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e4=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e5=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e6=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e7=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e8=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e9=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e10=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')
e11=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3,show='*')


master.mainloop()


# In[512]:


master=Tk()
master.title('churn_predict_model')


master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.configure(bg='lightblue')


label=Label(master,text='Bank churn',font=('Arial',20,'bold'),width=10,height=1,bg='lightblue',foreground='yellow').grid(row=0,columnspan=2)

Label(master,text='creditscore').grid(row=1)
Label(master,text='gender[0-1]').grid(row=2)
Label(master,text='age').grid(row=3)
Label(master,text='tenure').grid(row=4)
Label(master,text='balance').grid(row=5)
Label(master,text='	numofproducts').grid(row=6)
Label(master,text='isactivemember').grid(row=7)
Label(master,text='estimatedsalary').grid(row=8)
Label(master,text='geography1').grid(row=9)
Label(master,text='geography_Germany').grid(row=10)
Label(master,text='geography_Spain').grid(row=11)


e1=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e2=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e3=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e4=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e5=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e6=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e7=Entry( master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e8=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e9=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e10=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e11=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
  

e1.grid(row=1,column=1)    
e2.grid(row=2,column=1)
e3.grid(row=3,column=1)
e4.grid(row=4,column=1)
e5.grid(row=5,column=1)
e6.grid(row=6,column=1)
e7.grid(row=7,column=1)
e8.grid(row=8,column=1)
e9.grid(row=9,column=1)
e10.grid(row=10,column=1)
e11.grid(row=11,column=1)

master.mainloop()


# In[513]:


def show_entry():
    p1=float(e1.get())
    p2=float(e2.get())
    p3=float(e3.get())
    p4=float(e4.get())
    p5=float(e5.get())
    p6=float(e6.get())
    p7=float(e6.get())
    p8=float(e6.get())
    p9=float(e6.get())
    p10=float(e6.get())
    p11=float(e6.get())

    model=joblib.load('churn_predict_model')
    result=model.predict([[p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11]])
    
    
    Label(master, text='Bank_churn').grid(row=12)
    Label(master,text=result).grid(row=13)
    
    
master=Tk()

master.title('churn_predict_model')


master.geometry('400x300')
master.minsize(200,200)
master.maxsize(600,600)
master.configure(bg='lightblue')


label=Label(master,text='Bank_churn',font=('Arial',20,'bold'),width=10,height=1,bg='lightblue',foreground='yellow').grid(row=0,columnspan=2)

Label(master,text='creditscore').grid(row=1)
Label(master,text='gender[0-1]').grid(row=2)
Label(master,text='age').grid(row=3)
Label(master,text='tenure').grid(row=4)
Label(master,text='balance').grid(row=5)
Label(master,text='	numofproducts').grid(row=6)
Label(master,text='isactivemember').grid(row=7)
Label(master,text='estimatedsalary').grid(row=8)
Label(master,text='geography1').grid(row=9)
Label(master,text='geography_Germany').grid(row=10)
Label(master,text='geography_Spain').grid(row=11)


e1=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e2=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e3=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e4=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e5=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e6=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e7=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)  
e8=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e9=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e10=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
e11=Entry(master,font=('Arial',14),bg='grey',fg='white',borderwidth=3)
  

e1.grid(row=1,column=1)    
e2.grid(row=2,column=1)
e3.grid(row=3,column=1)
e4.grid(row=4,column=1)
e5.grid(row=5,column=1)
e6.grid(row=6,column=1)
e7.grid(row=7,column=1)
e8.grid(row=8,column=1)
e9.grid(row=9,column=1)
e10.grid(row=10,column=1)
e11.grid(row=11,column=1)





Button(master,text='predict',command=show_entry,bg='pink',font=('Arial',20,'bold'),borderwidth=3,activebackground='blue').grid()







master.mainloop()


# In[ ]:




